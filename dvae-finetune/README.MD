# DVAE Fine-tuning for XTTS

This project focuses on fine-tuning the Discrete Variational Autoencoder (DVAE) model for use with the XTTS (Cross-lingual Text-to-Speech) system. The DVAE is a key component in the XTTS pipeline, responsible for learning a compact representation of audio data. By fine-tuning the DVAE on a specific dataset, we can improve the quality and expressiveness of the generated speech.

## Benefits of Fine-tuning DVAE

Fine-tuning the DVAE model offers several advantages:

1. **Improved Audio Quality**: By training the DVAE on a dataset that closely matches the target domain or speaker, the model learns to capture the nuances and characteristics specific to that dataset. This results in higher-quality audio output with better clarity and fidelity.

2. **Enhanced Expressiveness**: A fine-tuned DVAE can better represent the emotional and prosodic variations present in the training data. This allows for more expressive and natural-sounding speech synthesis.

3. **Language Adaptation**: Fine-tuning the DVAE on a dataset in a specific language helps the model adapt to the unique phonetic and acoustic properties of that language. This is particularly beneficial for languages that are underrepresented in the pre-trained DVAE.

## Integration with GPT-2 Training

The fine-tuned DVAE plays a crucial role in training the GPT-2 model for XTTS. Here's how it contributes to the overall training process:

1. **Audio Representation**: The DVAE encodes the raw audio data into a compact latent representation. This representation captures the essential features and characteristics of the audio while reducing its dimensionality.

2. **GPT-2 Input**: The latent representations generated by the fine-tuned DVAE serve as input to the GPT-2 model during training. The GPT-2 model learns to generate text conditioned on these audio representations.

3. **Improved Alignment**: By using a fine-tuned DVAE that better captures the specific characteristics of the training data, the alignment between the audio and text becomes more accurate. This leads to improved synchronization and coherence in the generated speech.

4. **Enhanced Quality**: The fine-tuned DVAE provides higher-quality audio representations, which in turn enables the GPT-2 model to generate more natural and expressive speech output.

## Installation

To set up the project for DVAE fine-tuning, follow these steps:

1. Clone the repository:
   ```
   git clone https://github.com/daswer123/xtts-finetune-tests
   cd dvae-finetune
   ```

2. Create a virtual environment and activate it:
   ```
   python -m venv venv
   source venv/bin/activate  # For Linux/Mac
   venv\Scripts\activate  # For Windows
   ```

3. Install the required dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Download the pre-trained DVAE model and mel-spectrogram statistics:
   - [dvae.pth](https://huggingface.co/coqui/XTTS-v2/resolve/main/dvae.pth?download=true)
   - [mel_stats.pth](https://huggingface.co/coqui/XTTS-v2/resolve/main/mel_stats.pth?download=true)

   Place these files in the `base_model` directory.

5. Prepare your dataset:
   - Create a directory for your dataset, e.g., `dataset_raw`.
   - Place all the audio files in the `dataset_raw` directory.

## Dataset Creation

To create the dataset for DVAE fine-tuning, use the `dataset_creator.py` script. Here are the available command-line flags:

- `--input_data`: Path to the input data directory containing the audio files (required).
- `--output_path`: Path to the output directory where the processed dataset will be saved (default: `dataset_ready`).
- `--language`: Language of the audio files (required).
- `--train_percent`: Percentage of data to use for training (default: 0.8).
- `--max_audio_length`: Maximum length of audio chunks in seconds (default: 11).
- `--sample_rate`: Sample rate of the audio files (default: 22050).

Example usage:

**simple**
```
python dataset_creator.py --input_data dataset_raw --language en
```

**complicated**
```
python dataset_creator.py --input_data dataset_raw --language en --train_percent 0.8 --max_audio_length 10 --sample_rate 22050
```

## Training

To fine-tune the DVAE model on your dataset, use the `train_dvae.py` script. Here are the available command-line flags:

- `--dvae_checkpoint`: Path to the pre-trained DVAE checkpoint (default: `./base_model/dvae.pth`).
- `--mel_norm_file`: Path to the mel normalization file (default: `./base_model/mel_stats.pth`).
- `--dataset_path`: Path to the custom dataset (default: `./dataset_ready`).
- `--language`: Language of the custom dataset (required).
- `--epochs`: Number of training epochs (default: 20).
- `--batch_size`: Batch size for training (default: 10).
- `--learning_rate`: Learning rate for the optimizer (default: 5e-05).
- `--num_workers`: Number of workers for data loading (default: 8).
- `--grad_clip_norm`: Gradient clipping norm value (default: 0.5).
- `--use_mixed_precision`: Enable mixed precision training (default: False).
- `--use_wandb`: Enable logging to Weights and Biases (default: False).
- `--save_every`: Save model checkpoint every N epochs (default: 10).

Example usage:

**simple**
```
python train_dvae.py --dataset_path dataset_ready --language en
```

**complicated**
```
python train_dvae.py --dataset_path dataset_ready --language en --epochs 20 --batch_size 8 --learning_rate 5e-05 --use_wandb --save_every 5
```

After training, the best model checkpoint will be saved as `best_dvae_<language>.pth` in the output directory. You can use this fine-tuned DVAE model for further experiments or integrate it into the XTTS pipeline.
